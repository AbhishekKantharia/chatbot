import os
import streamlit as st
import google.generativeai as genai
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema.runnable import RunnableLambda
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from PyPDF2 import PdfReader
import docx2txt
import socket

# ========================= CONFIGURE STREAMLIT =========================
st.set_page_config(page_title="Smart AI Chatbot", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ Smart AI Chatbot")

# ========================= GOOGLE GEMINI AI CONFIG =========================
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("Google API key is missing! Set it as an environment variable.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)
llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.7)

# ========================= USER MANAGEMENT =========================
def get_user_ip():
    """Fetch user IP address (for banning feature)."""
    try:
        return socket.gethostbyname(socket.gethostname())
    except:
        return "Unknown"

user_ip = get_user_ip()
BANNED_IPS = ["192.168.1.100", "203.0.113.45"]  # Add banned IPs here

if user_ip in BANNED_IPS:
    st.error("üö´ You have been banned from using this chatbot.")
    st.stop()

# ========================= MANAGE MULTIPLE CHATS =========================
st.sidebar.header("üí¨ Manage Chats")
if "chats" not in st.session_state:
    st.session_state["chats"] = {}  # Store multiple chat sessions

chat_list = list(st.session_state["chats"].keys()) or ["New Chat"]
chat_name = st.sidebar.selectbox("Select a Chat", chat_list)

if st.sidebar.button("‚ûï Start New Chat"):
    new_chat_name = f"Chat {len(st.session_state['chats']) + 1}"
    st.session_state["chats"][new_chat_name] = {"messages": [], "context_docs": []}
    chat_name = new_chat_name

if chat_name not in st.session_state["chats"]:
    st.session_state["chats"][chat_name] = {"messages": [], "context_docs": []}

chat_session = st.session_state["chats"][chat_name]
messages = chat_session["messages"]

# ========================= DOCUMENT UPLOAD FOR RAG =========================
st.sidebar.header("üìÇ Upload Documents for RAG")
uploaded_file = st.sidebar.file_uploader("Upload PDF, DOCX, or TXT", type=["pdf", "docx", "txt"])

if uploaded_file:
    def extract_text(file):
        try:
            if file.name.endswith(".pdf"):
                reader = PdfReader(file)
                return " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
            elif file.name.endswith(".docx"):
                return docx2txt.process(file)
            else:
                return file.read().decode("utf-8")
        except Exception as e:
            st.sidebar.error(f"Error processing file: {str(e)}")
            return ""

    extracted_text = extract_text(uploaded_file)
    if extracted_text:
        chat_session["context_docs"].append(extracted_text)
        st.sidebar.success("‚úÖ Document added to chatbot knowledge!")

# ========================= PROCESS RAG =========================
retriever = None
if chat_session["context_docs"]:
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.create_documents(chat_session["context_docs"])
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_documents(docs, embeddings)
        retriever = vector_store.as_retriever()
    except Exception as e:
        st.sidebar.error(f"‚ùå Error setting up RAG: {str(e)}")

# ========================= CHATBOT THEME SELECTION =========================
theme = st.sidebar.selectbox("üé® Choose Chatbot Theme", ["Default", "Business", "Casual", "Legal"])

def generate_response(prompt, theme):
    """Modify chatbot tone based on theme selection."""
    if theme == "Business":
        return f"üìä Professional Response: {prompt}"
    elif theme == "Casual":
        return f"üòé Chill Response: {prompt}"
    elif theme == "Legal":
        return f"‚öñÔ∏è Legal Analysis: {prompt}"
    return f"ü§ñ Default: {prompt}"

# ========================= DISPLAY CHAT MESSAGES =========================
st.subheader(f"üí¨ {chat_name}")
for i, message in enumerate(messages):
    role = message["role"]
    content = message["content"]

    with st.chat_message(role):
        col1, col2 = st.columns([0.9, 0.1])
        col1.markdown(content)

        if col2.button("‚ùå", key=f"delete_{i}"):
            del messages[i]
            st.rerun()

# ========================= CHAT INPUT =========================
if prompt := st.chat_input("Ask me anything..."):
    messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    response_text = ""

    # Use RAG if context is available, else only AI
    if retriever:
        try:
            memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            retrieval_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)

            def process_input(input_text):
                return retrieval_chain.invoke(input_text)

            rag_pipeline = RunnableLambda(process_input)

            with st.chat_message("assistant"):
                response_container = st.empty()

                for chunk in rag_pipeline.stream(prompt):
                    if isinstance(chunk, str):
                        response_text += chunk
                    elif hasattr(chunk, "text"):
                        response_text += chunk.text
                    elif hasattr(chunk, "content"):
                        response_text += chunk.content

                response_container.markdown(response_text)

        except Exception as e:
            st.error(f"‚ùå Error processing response: {str(e)}")
            response_text = "I encountered an error while generating a response."

    else:
        try:
            response = llm.invoke(prompt)
            response_text = response.content if response else "I couldn't generate a response."

            with st.chat_message("assistant"):
                st.markdown(response_text)
        except Exception as e:
            st.error(f"‚ùå AI Error: {str(e)}")
            response_text = "I encountered an error while generating a response."

    # Store assistant response
    messages.append({"role": "assistant", "content": response_text})

# ========================= DELETE CHAT =========================
if st.sidebar.button("üóëÔ∏è Delete Chat"):
    del st.session_state["chats"][chat_name]
    st.rerun()
